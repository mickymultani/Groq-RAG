{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPamuQgsVTha5Y0XL9dMN64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickymultani/Groq-RAG/blob/main/GROQ_RAG_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq langchain langchain-core langchain-groq chromadb pypdf gradio"
      ],
      "metadata": {
        "id": "zlKVdjMh_XM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "iS5z9MDT_ygd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing Ollama per the above cell, open the notebook terminal from the bottom left and start ollama by entering `ollama serve`. The following cells with ollama commands ***will only work*** if ollama is running!"
      ],
      "metadata": {
        "id": "ua72mxG2Hi6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "W07TkVrPD8g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ri6ydX5e_R3L"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community import embeddings\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import time\n",
        "import textwrap\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"data\")\n",
        "the_text = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(the_text)"
      ],
      "metadata": {
        "id": "18lJKz90fDX9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For text files\n",
        "# file_path = r'data/YourTextFileName.txt'\n",
        "# loader = TextLoader(file_path)\n",
        "# the_text = loader.load()\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "# chunks = text_splitter.split_documents(the_text)"
      ],
      "metadata": {
        "id": "xf_4hDUREuq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    collection_name=\"ollama_embeds\",\n",
        "    embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "izMS2O_TE0mB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "VJf_QWGgLG2E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "            groq_api_key=groq_api_key,\n",
        "            model_name='mixtral-8x7b-32768'\n",
        "    )"
      ],
      "metadata": {
        "id": "ZRwArSOaACQS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "sfN4QX1XFVjP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the architecture with a simple hard coded question\n",
        "response = rag_chain.invoke(\"What is this document about\")\n",
        "print(textwrap.fill(response, width=80))"
      ],
      "metadata": {
        "id": "jDqTM5tMm4dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the questions dynamic using a chat interface. Let's use gradio for this.\n",
        "def process_question(user_question):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Directly using the user's question as input for rag_chain.invoke\n",
        "    response = rag_chain.invoke(user_question)\n",
        "\n",
        "    # Measure the response time\n",
        "    end_time = time.time()\n",
        "    response_time = f\"Response time: {end_time - start_time:.2f} seconds.\"\n",
        "\n",
        "    # Combine the response and the response time into a single string\n",
        "    full_response = f\"{response}\\n\\n{response_time}\"\n",
        "\n",
        "    return full_response\n",
        "\n",
        "# Setup the Gradio interface\n",
        "iface = gr.Interface(fn=process_question,\n",
        "                     inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
        "                     outputs=gr.Textbox(),\n",
        "                     title=\"GROQ CHAT\",\n",
        "                     description=\"Ask any question about your document, and get an answer along with the response time.\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "x5SfJrZjlPOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}